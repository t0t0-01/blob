{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "60c2dbdc",
   "metadata": {},
   "source": [
    "<img src=\"./images/Blob_logo.png\" width=600 height=600/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6727b6c5",
   "metadata": {},
   "source": [
    "# Table of Contents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "454f1971",
   "metadata": {},
   "source": [
    "1. [Introduction](#Introduction)\n",
    "2. [Methodology](#Methodology)\n",
    "3. [Implementation](#Implementation)\n",
    "4. [Deployment](#Deployment)\n",
    "5. [References](#References)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc0f53c",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff277e6",
   "metadata": {},
   "source": [
    "As part of our pipeline, we analyze users' conversation to determine social areas of development and provide personal feedback to our users."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d8fc215",
   "metadata": {},
   "source": [
    "<img src=\"./images/content_tracker.png\" height=800 width=800 />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "161dd86d",
   "metadata": {},
   "source": [
    "Conversation analysis contains two main components:\n",
    "1. **Facial Emotion Recognition**\n",
    "2. **Speech Analysis**\n",
    "\n",
    "In this notebook, we will be tackling the *Speech Analysis* part"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a84423",
   "metadata": {},
   "source": [
    "<img src=\"./images/analysis_tracker.png\" height=500 width=500 />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5366750",
   "metadata": {},
   "source": [
    "# Methodology"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c432e4",
   "metadata": {},
   "source": [
    "The service will be implemented as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f02db0e1",
   "metadata": {},
   "source": [
    "<img src=\"./images/diagram.png\" height=600 width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce3a380",
   "metadata": {},
   "source": [
    "\n",
    "## Social Areas of Development"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60cf4a88",
   "metadata": {},
   "source": [
    "Through our research, we were able to determine the following areas of development:\n",
    "\n",
    "1. Lack of empathy and understanding towards others\n",
    "2. Use of inappropriate or offensive language\n",
    "3. Interrupting or talking over others\n",
    "4. Failure to listen actively or to show interest in the conversation\n",
    "5. Failure to establish rapport or build a connection with the other person\n",
    "6. Talking excessively and dominating the conversation\n",
    "7. Bringing up sensitive or controversial topics without regard for the other person's feelings\n",
    "8. Engaging in negative or hostile behavior, such as insults or verbal attacks\n",
    "9. Dismissing or belittling the other person's perspective or experiences\n",
    "10. Failing to communicate clearly or effectively, resulting in misunderstandings or confusion."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e082936",
   "metadata": {},
   "source": [
    "For the purposes of this service, we will only be searching for 5 of these topics however:\n",
    "<img src=\"./images/development_topics.png\" height=800 width=800 />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d09e549",
   "metadata": {},
   "source": [
    "| Issue | Description | Importance | How to fix |\n",
    "| --- | --- | --- | --- |\n",
    "| Lack of empathy and understanding towards others | Difficulty in understanding and relating to other people's feelings, perspectives, and experiences. | Important as it can lead to misunderstandings, conflicts, and strained relationships. | Practicing active listening, perspective-taking, and showing genuine interest in others can help increase empathy and understanding towards others. Engaging in cultural diversity training can also help broaden one's perspectives and increase cultural competence. |\n",
    "| Use of inappropriate or offensive language | Using language that can be hurtful, offensive, or derogatory towards others. | Important as it can create a hostile or uncomfortable environment, hurt others' feelings, and damage relationships. | Practicing awareness of language use and avoiding using offensive or derogatory language towards others. Apologizing and making amends if offensive language is used. Engaging in cultural sensitivity training can also help broaden one's perspectives and increase cultural competence. |\n",
    "| Bringing up sensitive or controversial topics without regard for the other person's feelings | Initiating conversations or bringing up topics that may be sensitive or controversial without considering how the other person may feel about it. | Important as it can lead to discomfort, hurt feelings, or even conflict. | Practicing awareness of the other person's feelings and perspective before initiating conversations on sensitive or controversial topics. Asking for permission or checking in with the other person before bringing up such topics. Using a respectful and non-judgmental tone when discussing sensitive or controversial topics. |\n",
    "| Engaging in negative or hostile behavior, such as insults or verbal attacks | Using aggressive, hostile, or negative language towards others. | Important as it can create a hostile or uncomfortable environment, hurt others' feelings, and damage relationships. | Practicing awareness of one's behavior and language use towards others. Avoiding insults or verbal attacks towards others. Using a respectful and non-judgmental tone when communicating with others, even in times of conflict or disagreement. |\n",
    "| Dismissing or belittling the other person's perspective or experiences | Dismissing or belittling the other person's feelings, perspectives, or experiences. | Important as it can create a hostile or uncomfortable environment, hurt others' feelings, and damage relationships. | Practicing active listening and showing genuine interest in others' perspectives and experiences. Avoiding judgment or dismissive language when communicating with others. Using a respectful and non-judgmental tone when discussing differences in perspectives or experiences. |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "263fc9d7",
   "metadata": {},
   "source": [
    "# Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86fe0dfd",
   "metadata": {},
   "source": [
    "## Transcription Service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "23c5dd1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'upload_url': 'https://cdn.assemblyai.com/upload/ae8cf144-5150-483f-b13a-314c684743a8'}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "filename = r\"C:\\Users\\Anton\\Desktop\\Recording.m4a\"\n",
    "def read_file(filename, chunk_size=5242880):\n",
    "    with open(filename, 'rb') as _file:\n",
    "        while True:\n",
    "            data = _file.read(chunk_size)\n",
    "            if not data:\n",
    "                break\n",
    "            yield data\n",
    "\n",
    "headers = {'authorization': \"79fe1aefb66b45fca7e54ed5b2da42d3\"}\n",
    "response = requests.post('https://api.assemblyai.com/v2/upload',\n",
    "                        headers=headers,\n",
    "                        data=read_file(filename))\n",
    "\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5ba6b87e",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = response.json()['upload_url']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b95a2103",
   "metadata": {},
   "source": [
    "Let's queue for processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7f2e5f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint = \"https://api.assemblyai.com/v2/transcript\"\n",
    "\n",
    "json = {\n",
    "  \"audio_url\": url,\n",
    "  \"speaker_labels\": True\n",
    "\n",
    "}\n",
    "\n",
    "headers = {\n",
    "  \"Authorization\": \"79fe1aefb66b45fca7e54ed5b2da42d3\",\n",
    "  \"Content-Type\": \"application/json\"\n",
    "}\n",
    "\n",
    "response = requests.post(endpoint, json=json, headers=headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "72180135",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '6r1jywzwrj-3807-4e83-bd23-b61bc89db514',\n",
       " 'language_model': 'assemblyai_default',\n",
       " 'acoustic_model': 'assemblyai_default',\n",
       " 'language_code': 'en_us',\n",
       " 'status': 'queued',\n",
       " 'audio_url': 'https://cdn.assemblyai.com/upload/ae8cf144-5150-483f-b13a-314c684743a8',\n",
       " 'text': None,\n",
       " 'words': None,\n",
       " 'utterances': None,\n",
       " 'confidence': None,\n",
       " 'audio_duration': None,\n",
       " 'punctuate': True,\n",
       " 'format_text': True,\n",
       " 'dual_channel': None,\n",
       " 'webhook_url': None,\n",
       " 'webhook_status_code': None,\n",
       " 'webhook_auth': False,\n",
       " 'webhook_auth_header_name': None,\n",
       " 'speed_boost': False,\n",
       " 'auto_highlights_result': None,\n",
       " 'auto_highlights': False,\n",
       " 'audio_start_from': None,\n",
       " 'audio_end_at': None,\n",
       " 'word_boost': [],\n",
       " 'boost_param': None,\n",
       " 'filter_profanity': False,\n",
       " 'redact_pii': False,\n",
       " 'redact_pii_audio': False,\n",
       " 'redact_pii_audio_quality': None,\n",
       " 'redact_pii_policies': None,\n",
       " 'redact_pii_sub': None,\n",
       " 'speaker_labels': True,\n",
       " 'content_safety': False,\n",
       " 'iab_categories': False,\n",
       " 'content_safety_labels': {},\n",
       " 'iab_categories_result': {},\n",
       " 'language_detection': False,\n",
       " 'custom_spelling': None,\n",
       " 'throttled': None,\n",
       " 'auto_chapters': False,\n",
       " 'summarization': False,\n",
       " 'summary_type': None,\n",
       " 'summary_model': None,\n",
       " 'custom_topics': False,\n",
       " 'topics': [],\n",
       " 'disfluencies': False,\n",
       " 'sentiment_analysis': False,\n",
       " 'sentiment_analysis_results': None,\n",
       " 'chapters': None,\n",
       " 'entity_detection': False,\n",
       " 'entities': None,\n",
       " 'speakers_expected': None}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "86cef1c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "id = response.json()['id']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb59639",
   "metadata": {},
   "source": [
    "Let's get result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "858d70a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': '6r1jywzwrj-3807-4e83-bd23-b61bc89db514', 'language_model': 'assemblyai_default', 'acoustic_model': 'assemblyai_default', 'language_code': 'en_us', 'status': 'completed', 'audio_url': 'https://cdn.assemblyai.com/upload/ae8cf144-5150-483f-b13a-314c684743a8', 'text': \"Hey, man. How's it going? All doing good.\", 'words': [{'text': 'Hey,', 'start': 1610, 'end': 1774, 'confidence': 0.9992, 'speaker': 'A'}, {'text': 'man.', 'start': 1812, 'end': 2062, 'confidence': 0.5873, 'speaker': 'A'}, {'text': \"How's\", 'start': 2116, 'end': 2346, 'confidence': 0.93009, 'speaker': 'A'}, {'text': 'it', 'start': 2378, 'end': 2478, 'confidence': 0.99975, 'speaker': 'A'}, {'text': 'going?', 'start': 2484, 'end': 3134, 'confidence': 0.8719, 'speaker': 'A'}, {'text': 'All', 'start': 3332, 'end': 3694, 'confidence': 0.99361, 'speaker': 'A'}, {'text': 'doing', 'start': 3732, 'end': 3982, 'confidence': 0.99865, 'speaker': 'A'}, {'text': 'good.', 'start': 4036, 'end': 4140, 'confidence': 0.99971, 'speaker': 'A'}], 'utterances': [{'confidence': 0.92252625, 'end': 4140, 'speaker': 'A', 'start': 1610, 'text': \"Hey, man. How's it going? All doing good.\", 'words': [{'text': 'Hey,', 'start': 1610, 'end': 1774, 'confidence': 0.9992, 'speaker': 'A'}, {'text': 'man.', 'start': 1812, 'end': 2062, 'confidence': 0.5873, 'speaker': 'A'}, {'text': \"How's\", 'start': 2116, 'end': 2346, 'confidence': 0.93009, 'speaker': 'A'}, {'text': 'it', 'start': 2378, 'end': 2478, 'confidence': 0.99975, 'speaker': 'A'}, {'text': 'going?', 'start': 2484, 'end': 3134, 'confidence': 0.8719, 'speaker': 'A'}, {'text': 'All', 'start': 3332, 'end': 3694, 'confidence': 0.99361, 'speaker': 'A'}, {'text': 'doing', 'start': 3732, 'end': 3982, 'confidence': 0.99865, 'speaker': 'A'}, {'text': 'good.', 'start': 4036, 'end': 4140, 'confidence': 0.99971, 'speaker': 'A'}]}], 'confidence': 0.92252625, 'audio_duration': 5, 'punctuate': True, 'format_text': True, 'dual_channel': None, 'webhook_url': None, 'webhook_status_code': None, 'webhook_auth': False, 'webhook_auth_header_name': None, 'speed_boost': False, 'auto_highlights_result': None, 'auto_highlights': False, 'audio_start_from': None, 'audio_end_at': None, 'word_boost': [], 'boost_param': None, 'filter_profanity': False, 'redact_pii': False, 'redact_pii_audio': False, 'redact_pii_audio_quality': None, 'redact_pii_policies': None, 'redact_pii_sub': None, 'speaker_labels': True, 'content_safety': False, 'iab_categories': False, 'content_safety_labels': {'status': 'unavailable', 'results': [], 'summary': {}}, 'iab_categories_result': {'status': 'unavailable', 'results': [], 'summary': {}}, 'language_detection': False, 'custom_spelling': None, 'throttled': None, 'auto_chapters': False, 'summarization': False, 'summary_type': None, 'summary_model': None, 'custom_topics': False, 'topics': [], 'disfluencies': False, 'sentiment_analysis': False, 'chapters': None, 'sentiment_analysis_results': None, 'entity_detection': False, 'entities': None, 'summary': None, 'speakers_expected': None}\n"
     ]
    }
   ],
   "source": [
    "endpoint = f\"https://api.assemblyai.com/v2/transcript/{id}\"\n",
    "headers = {\n",
    "    \"authorization\": \"79fe1aefb66b45fca7e54ed5b2da42d3\",\n",
    "}\n",
    "response = requests.get(endpoint, headers=headers)\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "eb7e8bef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'confidence': 0.9225575000000001,\n",
       "  'end': 4140,\n",
       "  'speaker': 'A',\n",
       "  'start': 1610,\n",
       "  'text': \"Hey, man. How's it going? All doing good.\",\n",
       "  'words': [{'text': 'Hey,',\n",
       "    'start': 1610,\n",
       "    'end': 1774,\n",
       "    'confidence': 0.9992,\n",
       "    'speaker': 'A'},\n",
       "   {'text': 'man.',\n",
       "    'start': 1812,\n",
       "    'end': 2062,\n",
       "    'confidence': 0.5873,\n",
       "    'speaker': 'A'},\n",
       "   {'text': \"How's\",\n",
       "    'start': 2116,\n",
       "    'end': 2346,\n",
       "    'confidence': 0.93009,\n",
       "    'speaker': 'A'},\n",
       "   {'text': 'it',\n",
       "    'start': 2378,\n",
       "    'end': 2478,\n",
       "    'confidence': 1.0,\n",
       "    'speaker': 'A'},\n",
       "   {'text': 'going?',\n",
       "    'start': 2484,\n",
       "    'end': 3134,\n",
       "    'confidence': 0.8719,\n",
       "    'speaker': 'A'},\n",
       "   {'text': 'All',\n",
       "    'start': 3332,\n",
       "    'end': 3694,\n",
       "    'confidence': 0.99361,\n",
       "    'speaker': 'A'},\n",
       "   {'text': 'doing',\n",
       "    'start': 3732,\n",
       "    'end': 3982,\n",
       "    'confidence': 0.99865,\n",
       "    'speaker': 'A'},\n",
       "   {'text': 'good.',\n",
       "    'start': 4036,\n",
       "    'end': 4140,\n",
       "    'confidence': 0.99971,\n",
       "    'speaker': 'A'}]}]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utterances = response.json()['utterances']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee7b1db",
   "metadata": {},
   "source": [
    "## Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db84a2be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[0, 0, 1, 0, 0, 1]']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import openai\n",
    "\n",
    "openai.api_key = \"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\"\n",
    "\n",
    "prompt = \"\"\"You are a communication and social expert. Your job is to identify issues in people's conversations. You will be given a text extract and you need to identify if one of these issues were present in the conversation:\n",
    "\n",
    "- Lack of empathy and understanding towards others\n",
    "- Use of inappropriate or offensive language\n",
    "- Bringing up sensitive or controversial topics without regard for the other person's feelings\n",
    "- Engaging in negative or hostile behavior, such as insults or verbal attacks\n",
    "- Dismissing or belittling the other person's perspective or experiences\n",
    "\n",
    "Note that each one could have more than one issue. Return ONLY a vector with 1 if the issue is present, and 0 otherwise. Don't name the issue, just use a vector where the dimensions follow the order of the issues as i gave them to you. Don't return anything other than this vector. Don't explain anything. Don't include any text other than the vector.\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "conversations = [\"Hey man, how are you doing? All's good? Yeah man, all is good. And you? All's good too.  So I hear you graduated as a computer engineer from LAU. What are you doing now? I'm actually working at CompanyX Oh, what's that? Never heard of it...  It's a small development firm. Were you not able to secure a job at Google? that's funny. I actually work at Google. It is what it is man. Great for you.\"]\n",
    "\n",
    "labels = []\n",
    "\n",
    "for conv in conversations:\n",
    "    response = openai.ChatCompletion.create(\n",
    "          model=\"gpt-3.5-turbo\",\n",
    "          messages=[\n",
    "                {\"role\": \"system\", \"content\": prompt},\n",
    "                {\"role\": \"user\", \"content\": conv},\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    labels.append( response['choices'][0]['message']['content'])\n",
    "    \n",
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89857469",
   "metadata": {},
   "source": [
    "### Speakers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6048d04a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['{Speaker A: [0, 0, 0, 0, 0], Speaker B: [1, 1, 1, 1, 1]}']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import openai\n",
    "\n",
    "openai.api_key = \"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\"\n",
    "\n",
    "prompt = \"\"\"You are a communication and social expert. You will be given a text extract that contains a conversation between two speakers. Your job is to identify each speaker's communication weaknesses. The weaknesses can be one of the following:\n",
    "\n",
    "- Lack of empathy and understanding towards others\n",
    "- Use of inappropriate or offensive language\n",
    "- Bringing up sensitive or controversial topics without regard for the other person's feelings\n",
    "- Engaging in negative or hostile behavior, such as insults or verbal attacks\n",
    "- Dismissing or belittling the other person's perspective or experiences\n",
    "\n",
    "The text you will be given will be labeled according to speaker. You need to return exactly one vector for each speaker that takes into account all the things said by only this speaker.\n",
    "\n",
    "The vector will have a 1 if the issue is present, and 0 otherwise. Don't name the issue, just use a vector where the dimensions follow the order of the issues as i gave them to you. Note that each one could have more than one issue. \n",
    "\n",
    "Make sure not to confuse the speakers. Pay very close attention to who is exhibiting the issue and put the flag in that speaker's vector. Don't put it in the others'. If B exhibited an issue towards A (for instance, if B was insensitive to A), then the 1 would appear in B's vector. It would not appear in A's. \n",
    "\n",
    "Make sure your output follows the following format: '{Speaker A: [x, x, x, x, x], Speaker B: [x, x, x, x, x]}'\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "conversations = [\"\"\"\n",
    "\n",
    "Person A: \"I'm really struggling with this project. I feel like I'm not making any progress.\"\n",
    "Person B: \"Well, have you tried working harder? Maybe you just need to put in more effort.\"\n",
    "\n",
    "Person A: \"I have been working really hard, but I just can't seem to get the results I want.\"\n",
    "Person B: \"That's because you're not smart enough. Some people just don't have what it takes.\"\n",
    "\n",
    "Person A: \"That's not fair. I'm doing the best I can with the resources I have.\"\n",
    "Person B: \"Well, maybe you should have planned better or asked for help earlier. It's not my problem if you can't handle the pressure.\"\n",
    "\n",
    "Person A: \"I really need some support right now. I feel like I'm drowning in this project and no one is listening to me.\"\n",
    "Person B: \"I understand that you're struggling, but I have my own problems to deal with. I don't have time to listen to your complaints.\"\n",
    "\n",
    "Person A: \"I'm sorry if I'm being a burden. I just really need some help right now.\"\n",
    "Person B: \"Well, if you can't handle the pressure, maybe this isn't the right job for you.\"\n",
    "\n",
    "\"\"\"]\n",
    "\n",
    "labels = []\n",
    "\n",
    "for conv in conversations:\n",
    "    response = openai.ChatCompletion.create(\n",
    "          model=\"gpt-3.5-turbo\",\n",
    "          messages=[\n",
    "                {\"role\": \"system\", \"content\": prompt},\n",
    "                {\"role\": \"user\", \"content\": conv},\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    labels.append( response['choices'][0]['message']['content'])\n",
    "    \n",
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c0f105e",
   "metadata": {},
   "source": [
    "# Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0025998",
   "metadata": {},
   "source": [
    "For deployment, refer to the *src* folder of the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "653866e7",
   "metadata": {},
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51327ed7",
   "metadata": {},
   "source": [
    "https://www.assemblyai.com/docs/walkthroughs#getting-the-transcription-result  \n",
    "https://www.assemblyai.com/blog/the-top-free-speech-to-text-apis-and-open-source-engines/  \n",
    "End-to-End Speech Recognition: A Survey Rohit Prabhavalkar, Member, IEEE, Takaaki Hori, Senior Member, IEEE, Tara N. Sainath, Fellow, IEEE, Ralf Schluter, ¨ Senior Member, IEEE, and Shinji Watanabe, Fellow, IEEE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
